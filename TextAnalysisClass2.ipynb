{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNX9/pNNewURlovmOuPcZDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfindiamooc/mlp/blob/main/TextAnalysisClass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson #2: From Words to Numbers - Bag of Words and TF-IDF\n",
        "\n",
        "Welcome to Lesson #2! In the last lesson, you learned how to preprocess text using `CountVectorizer`. Now, we'll focus on a few more methods of **text vectorization** - turning words into numbers that machine learning models can understand.\n",
        "\n",
        "We'll cover:\n",
        "\n",
        "*   **Bag of Words (BoW):**  A simple but fundamental technique.\n",
        "*   **N-grams:**  Capturing some word order.\n",
        "*   **TF-IDF:**  Weighting words by importance.\n",
        "\n",
        "Let's dive into the code!"
      ],
      "metadata": {
        "id": "SPtgcfHBZifa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 1: Basic BoW Code (from Lesson 1 Recap)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Sachin Tendulkar is a legendary batsman.\",\n",
        "    \"India won the Cricket World Cup in 2011.\",\n",
        "    \"RRR is a blockbuster Indian movie.\",\n",
        "    \"Bollywood movies are very entertaining.\",\n",
        "    \"Is Sachin Tendulkar the greatest batsman?\",\n",
        "]\n",
        "\n",
        "vectorizer_bow = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    token_pattern=r'[a-zA-Z]+',\n",
        "    stop_words='english')\n",
        "vectorizer_bow.fit(corpus)\n",
        "X_bow = vectorizer_bow.transform(corpus)\n",
        "\n",
        "print(\"Bag of Words Vocabulary:\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(\"\\nBag of Words Document-Term Matrix (Counts):\")\n",
        "print(X_bow.toarray())"
      ],
      "metadata": {
        "id": "9fvfKdxNZfux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab3440f-3c2f-42a9-eead-700358bb69e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Vocabulary:\n",
            "['batsman' 'blockbuster' 'bollywood' 'cricket' 'cup' 'entertaining'\n",
            " 'greatest' 'india' 'indian' 'legendary' 'movie' 'movies' 'rrr' 'sachin'\n",
            " 'tendulkar' 'won' 'world']\n",
            "\n",
            "Bag of Words Document-Term Matrix (Counts):\n",
            "[[1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0]\n",
            " [0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1]\n",
            " [0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (BoW) in Detail\n",
        "\n",
        "Remember this code from the last lesson? Let's break down what's happening with **Bag of Words (BoW)**:\n",
        "\n",
        "*   **Vocabulary Creation:** `CountVectorizer` first builds a **vocabulary**. This is just a list of all the unique words in your text data, after preprocessing (like lowercasing and removing stop words).  Look at the \"Vocabulary\" output above - these are the words `CountVectorizer` learned from our `corpus`.\n",
        "\n",
        "*   **Document-Term Matrix:**  Then, `CountVectorizer` creates a **Document-Term Matrix**.\n",
        "    *   Each **row** in this matrix represents a **document** (in our case, each sentence in the `corpus`).\n",
        "    *   Each **column** represents a **word** from the **vocabulary**.\n",
        "    *   The **numbers** in the matrix are simply the **counts** of each word in each document.\n",
        "\n",
        "*   **\"Bag of Words\" Concept:**  The name \"Bag of Words\" comes from the fact that we **lose the order of words**.  We only care about *which words are present* and *how often* they appear in each document.  Word order is ignored.\n",
        "\n",
        "Let's move on to N-grams to capture some word order!"
      ],
      "metadata": {
        "id": "SE7zuAQZZs1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing N-grams\n",
        "\n",
        "**Problem with basic BoW:**  BoW treats phrases like \"good movie\" and \"movie good\" as the same thing because it ignores word order.  Sometimes, word order *does* matter for meaning!\n",
        "\n",
        "**Solution: N-grams!** N-grams are sequences of N words that can capture some word order. Let's start with **bigrams** (N=2), which are pairs of words."
      ],
      "metadata": {
        "id": "9xDaZskhZ97A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 2: N-gram Code Example (Bigrams)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"good movie\",\n",
        "    \"movie good\",\n",
        "    \"not good movie\",\n",
        "]\n",
        "\n",
        "vectorizer_bigram = CountVectorizer(\n",
        "    ngram_range=(2, 2), # ngram_range=(2, 2) for bigrams\n",
        "    lowercase=True,\n",
        "    token_pattern=r'[a-zA-Z]+',\n",
        "    stop_words='english')\n",
        "vectorizer_bigram.fit(corpus)\n",
        "X_bigram = vectorizer_bigram.transform(corpus)\n",
        "\n",
        "print(\"Bigram Vocabulary:\")\n",
        "print(vectorizer_bigram.get_feature_names_out())\n",
        "print(\"\\nBigram Document-Term Matrix:\")\n",
        "print(X_bigram.toarray())"
      ],
      "metadata": {
        "id": "TQMy7qAZZ61y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d177606-46c5-47dd-ce1a-419d51abafef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Vocabulary:\n",
            "['good movie' 'movie good']\n",
            "\n",
            "Bigram Document-Term Matrix:\n",
            "[[1 0]\n",
            " [0 1]\n",
            " [1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of N-grams (Bigrams)\n",
        "\n",
        "Look at the code and output above.  The key change is:\n",
        "\n",
        "`vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), ...)`\n",
        "\n",
        "*   **`ngram_range=(2, 2)`:** This is what tells `CountVectorizer` to use **bigrams** (sequences of 2 words).\n",
        "\n",
        "Run the code and check the output:\n",
        "\n",
        "*   **Bigram Vocabulary:** Notice the vocabulary now contains word pairs like `\"good movie\"` and `\"movie good\"`.  These are treated as distinct features!\n",
        "\n",
        "*   **Bigram Document-Term Matrix:** The matrix now reflects the counts of these bigrams in each document.\n",
        "\n",
        "By using bigrams, we've captured a bit of word order information that was lost in basic BoW.\n",
        "\n",
        "Now, let's try using both unigrams (single words) and bigrams together!"
      ],
      "metadata": {
        "id": "wsDe6SyhaDfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: N-gram Code Example (Unigrams and Bigrams)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"good movie\",\n",
        "    \"movie good\",\n",
        "    \"not good movie\",\n",
        "]\n",
        "\n",
        "vectorizer_unigram_bigram = CountVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    lowercase=True,\n",
        "    token_pattern=r'[a-zA-Z]+',\n",
        "    stop_words='english') # ngram_range=(1, 2) for unigrams and bigrams\n",
        "vectorizer_unigram_bigram.fit(corpus)\n",
        "X_unigram_bigram = vectorizer_unigram_bigram.transform(corpus)\n",
        "\n",
        "print(\"\\nUnigram and Bigram Vocabulary:\")\n",
        "print(vectorizer_unigram_bigram.get_feature_names_out())\n",
        "print(\"\\nUnigram and Bigram Document-Term Matrix:\")\n",
        "print(X_unigram_bigram.toarray())"
      ],
      "metadata": {
        "id": "V83Jf8u0aLIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea095e3-b1d1-404f-b15f-94b948bf6110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unigram and Bigram Vocabulary:\n",
            "['good' 'good movie' 'movie' 'movie good']\n",
            "\n",
            "Unigram and Bigram Document-Term Matrix:\n",
            "[[1 1 1 0]\n",
            " [1 0 1 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Unigrams and Bigrams (`ngram_range=(1, 2)`)\n",
        "\n",
        "In this code, we changed `ngram_range` to:\n",
        "\n",
        "`vectorizer_unigram_bigram = CountVectorizer(ngram_range=(1, 2), ...)`\n",
        "\n",
        "*   **`ngram_range=(1, 2)`:**  This tells `CountVectorizer` to include **both unigrams (single words) AND bigrams (pairs of words)** in the vocabulary.\n",
        "\n",
        "Run this code and look at the output:\n",
        "\n",
        "*   **Unigram and Bigram Vocabulary:** The vocabulary now has both individual words (like \"good\", \"movie\") and word pairs (like \"good movie\", \"movie good\").\n",
        "\n",
        "*   **Unigram and Bigram Document-Term Matrix:** The matrix counts both unigrams and bigrams.\n",
        "\n",
        "Using `ngram_range=(1, 2)` or higher allows you to capture more context than just single words. However, be aware that the vocabulary can become much larger as you increase the `ngram_range`!\n",
        "\n",
        "Next, let's learn about TF-IDF, which weights words based on their importance."
      ],
      "metadata": {
        "id": "6tg0CpwyaGfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF - Weighting Words by Importance\n",
        "\n",
        "**Problem with BoW and N-grams:**  Common words like \"movie\", \"sentence\", \"document\", \"is\", \"the\", etc., might appear frequently in *all* documents. Are these words really the most important for understanding what a document is *about*?  Probably not.\n",
        "\n",
        "**Solution: TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "\n",
        "TF-IDF is a technique that **weights words based on their importance**. It gives higher weights to words that are:\n",
        "\n",
        "*   **Frequent in a specific document (Term Frequency - TF)**\n",
        "*   **Rare across *all* documents in the corpus (Inverse Document Frequency - IDF)**\n",
        "\n",
        "Let's see TF-IDF in action!"
      ],
      "metadata": {
        "id": "-pOdc9nuaWyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 4: TF-IDF Code Example\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Sachin Tendulkar is a legendary batsman.\",\n",
        "    \"India won the Cricket World Cup in 2011.\",\n",
        "    \"RRR is a blockbuster Indian movie.\",\n",
        "    \"Bollywood movies are very entertaining.\",\n",
        "    \"Is Sachin Tendulkar the greatest batsman?\",\n",
        "]\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer(lowercase=True, token_pattern=r'[a-zA-Z]+', stop_words='english')\n",
        "vectorizer_tfidf.fit(corpus)\n",
        "X_tfidf = vectorizer_tfidf.transform(corpus)\n",
        "\n",
        "print(\"TF-IDF Vocabulary:\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Document-Term Matrix:\")\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "Whbjj5n6aQzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093fcd7b-c9f1-459f-b4e7-b8df20c1eba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vocabulary:\n",
            "['batsman' 'blockbuster' 'bollywood' 'cricket' 'cup' 'entertaining'\n",
            " 'greatest' 'india' 'indian' 'legendary' 'movie' 'movies' 'rrr' 'sachin'\n",
            " 'tendulkar' 'won' 'world']\n",
            "\n",
            "TF-IDF Document-Term Matrix:\n",
            "[[0.4695148  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.5819515  0.         0.\n",
            "  0.         0.4695148  0.4695148  0.         0.        ]\n",
            " [0.         0.         0.         0.4472136  0.4472136  0.\n",
            "  0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.4472136  0.4472136 ]\n",
            " [0.         0.5        0.         0.         0.         0.\n",
            "  0.         0.         0.5        0.         0.5        0.\n",
            "  0.5        0.         0.         0.         0.        ]\n",
            " [0.         0.         0.57735027 0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.4695148  0.         0.         0.         0.         0.\n",
            "  0.5819515  0.         0.         0.         0.         0.\n",
            "  0.         0.4695148  0.4695148  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of TF-IDF\n",
        "\n",
        "In this code, we simply replaced `CountVectorizer` with `TfidfVectorizer`:\n",
        "\n",
        "`vectorizer_tfidf = TfidfVectorizer(...)`\n",
        "\n",
        "Run the code and compare the output to the BoW output from earlier.\n",
        "\n",
        "*   **TF-IDF Values:** Look at the numbers in the \"TF-IDF Document-Term Matrix\".  They are no longer just counts! They are **TF-IDF scores**.\n",
        "\n",
        "Let's understand TF and IDF:\n",
        "\n",
        "*   **TF (Term Frequency):**  This is basically the same as the word counts we saw in Bag of Words. It measures how often a word appears in a *particular document*.\n",
        "\n",
        "*   **IDF (Inverse Document Frequency):** This measures how rare a word is across the *entire corpus* (collection of documents).\n",
        "    *   Words that are very common in *all* documents (like \"is\", \"the\", \"document\") will have a *low IDF*.\n",
        "    *   Words that are rare and appear mainly in *specific* documents will have a *high IDF* .\n",
        "\n",
        "*   **TF-IDF Score = TF \\* IDF:** The TF-IDF score for a word in a document is calculated by multiplying its TF and IDF values.\n",
        "    *   Words that are frequent in a document *and* rare in the overall corpus will get **high TF-IDF scores**. These are likely the words that are most important for understanding the content of *that specific document*.\n",
        "\n",
        "**Compare BoW and TF-IDF Matrices:** If you compare the TF-IDF matrix to the BoW matrix (mentally or side-by-side), you might notice that common words like \"document\", \"first\", \"is\", \"the\", etc., tend to have lower values in the TF-IDF matrix, while words that are more specific to certain documents might have relatively higher values.  (This depends on the example corpus, but that's the general idea).\n",
        "\n",
        "TF-IDF is often a more effective way to represent text for machine learning than simple Bag of Words because it takes into account the importance of words within the context of the entire corpus.\n",
        "\n",
        "Now, let's experiment and compare these vectorization methods!"
      ],
      "metadata": {
        "id": "1oCYBpf7abZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimentation and Comparison\n",
        "\n",
        "Time to experiment and see how these vectorization techniques work! Try a few experiments."
      ],
      "metadata": {
        "id": "fjcG_a87ad4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 1a. `ngram_range` in `CountVectorizer`\n",
        "\n",
        "Try different `ngram_range` values in `CountVectorizer`.  For example, try `ngram_range`=(1, 3) to include unigrams, bigrams, and trigrams.  What happens to the vocabulary size and the document-term matrix when you increase the `ngram_range`?"
      ],
      "metadata": {
        "id": "gRgw1H2o500P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "n-SG2D-_5mzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 1b. `ngram_range` in `TfIdfVectorizer`\n",
        "\n",
        "Try different `ngram_range` values in `TfIdfVectorizer`.  For example, try `ngram_range`=(1, 3) to include unigrams, bigrams, and trigrams.  What happens to the vocabulary size and the document-term matrix when you increase the `ngram_range`?"
      ],
      "metadata": {
        "id": "LsnWvVXc6MBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "N2jgnuyv6M2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 2.  Different Corpora\n",
        ">   \n",
        "Change the `corpus` variable to use different text examples (e.g., movie reviews, news snippets, your own sentences).\n",
        "\n",
        "Observe how the BoW and TF-IDF matrices change with different text data.\n",
        "\n"
      ],
      "metadata": {
        "id": "wd8H6uYI6cEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "tYsBQ-W36vaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 3. Stop Word Removal:\n",
        "\n",
        ">\n",
        "What happens if you remove `stop_words='english'` in `TfidfVectorizer`?\n",
        "\n",
        "> Do common words like \"is\", \"the\", \"and\" get higher TF-IDF scores when you remove stop word filtering?"
      ],
      "metadata": {
        "id": "Gkrt_2CY7E76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "ZbcPwuc27PXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 4. Real-World Example (Qualitative)\n",
        "*   Think about a real-world text classification task, like classifying movie reviews as positive or negative.\n",
        "\n",
        "*   Which words do you think TF-IDF would weight highly in a **positive** movie review?\n",
        "\n",
        "*   Which words would TF-IDF weight highly in a **negative** movie review?\n",
        "\n",
        "*   Discuss qualitatively - you don't need to code this part, just think about it!"
      ],
      "metadata": {
        "id": "3uI8Dl9N7UlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After experimenting, read the comparison summary below."
      ],
      "metadata": {
        "id": "Tw6WZHyB7gl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Summary: BoW, N-grams, TF-IDF\n",
        "\n",
        "Let's summarize the vectorization techniques we've learned:\n",
        "\n",
        "*   **Bag of Words (BoW):**\n",
        "    *   **Pros:** Simple, easy to understand, computationally fast.\n",
        "    *   **Cons:** Ignores word order, treats all words equally (doesn't account for word importance).\n",
        "\n",
        "*   **N-grams:**\n",
        "    *   **Pros:** Captures some word order information, can improve performance in some cases.\n",
        "    *   **Cons:** Vocabulary can become very large very quickly (especially for higher N values), still mostly loses word order beyond the N-gram window.\n",
        "\n",
        "*   **TF-IDF:**\n",
        "    *   **Pros:** Weights words by importance (frequency in document AND rarity in corpus), often performs better than simple BoW in practice.\n",
        "    *   **Cons:** Still a Bag-of-Words approach - word order is mostly lost. Can be slightly more computationally expensive than BoW.\n",
        "\n",
        "**Which vectorization method is \"best\"?**\n",
        "\n",
        "There's no single \"best\" method for all text tasks!  It depends on your specific data and problem.\n",
        "\n",
        "*   For simple tasks or as a starting point, **BoW** can be a good baseline.\n",
        "*   If word order is potentially important, **N-grams** might help.\n",
        "*   For many text classification tasks, **TF-IDF** often provides a good balance of simplicity and performance.\n",
        "\n",
        "In the next lessons, we'll experiment with these vectorization methods when we build machine learning models and see how they affect model performance!"
      ],
      "metadata": {
        "id": "9PhJlg_oamLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary and Next Steps\n",
        "\n",
        "Great job! In this lesson, you learned about **text vectorization** and explored three important techniques:\n",
        "\n",
        "*   **Bag of Words (BoW)**\n",
        "*   **N-grams**\n",
        "*   **TF-IDF**\n",
        "\n",
        "You used `CountVectorizer` and `TfidfVectorizer` in scikit-learn to create numerical representations of text.\n",
        "\n",
        "**Key Takeaway:** You now have different ways to transform text into numbers that machine learning models can understand.  You can choose the vectorization method that is most appropriate for your text analysis task.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "In the next lesson, we'll finally start building **machine learning models for text classification**! We'll use the vector representations we learned today and apply models like **Logistic Regression** and **Decision Trees** to classify text documents. Get ready to build your first text classifiers!"
      ],
      "metadata": {
        "id": "i-HGG3Ngaquv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways for Lesson #2 (for students):\n",
        "\n",
        "*   **Text vectorization** is essential to use text data in machine learning.\n",
        "*   **Bag of Words (BoW)** is a basic method that counts word frequencies.\n",
        "*   **N-grams** extend BoW to capture some word order.\n",
        "*   **TF-IDF** weights words by their importance in a document and corpus.\n",
        "*   `CountVectorizer` and `TfidfVectorizer` in scikit-learn are used to implement these techniques."
      ],
      "metadata": {
        "id": "A4RMc_Zparyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resources for Lesson #2:\n",
        "\n",
        "*   **Scikit-learn documentation on `CountVectorizer`:** [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "*   **Scikit-learn documentation on `TfidfVectorizer`:** [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ],
      "metadata": {
        "id": "5YkmlBFpawnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Notes:\n",
        "\n",
        "*   **Customization:**  Remember that `CountVectorizer` and `TfidfVectorizer` have many more parameters you can explore (e.g., `max_features`, custom tokenizers, etc.).  Check the scikit-learn documentation for details!\n",
        "\n",
        "*   **Choosing Vectorizer:**  The \"best\" vectorizer often depends on your specific text data and task. Experimentation is key!  We'll see this in later lessons when we build models and compare performance with different vectorizers.\n",
        "\n",
        "*   **Beyond TF-IDF:**  TF-IDF is a classic and effective technique, but there are more advanced vectorization methods, such as word embeddings (Word2Vec, GloVe, FastText), which we'll cover in later lessons. These methods capture semantic meaning and relationships between words in a richer way than BoW or TF-IDF."
      ],
      "metadata": {
        "id": "u12HAcSza2mf"
      }
    }
  ]
}