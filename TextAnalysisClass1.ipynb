{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvywGqkhwPoWs5LSV0p5pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfindiamooc/mlp/blob/main/TextAnalysisClass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson #1: Getting Started with Text Data - Preprocessing Basics in Scikit-learn"
      ],
      "metadata": {
        "id": "Cp7QZQ4FwVae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to expect:\n",
        "\n",
        "* **Top-Down & Code-First**: We'll jump into code immediately and then explain the concepts as we go. Students will see working code from the first few minutes.\n",
        "* **Start Simple, Iterate**: We begin with very basic preprocessing and gradually add more steps.\n",
        "Real-World Relevance (Implicit): We'll frame text data as something encountered in everyday applications (reviews, articles, etc.), and the practical exercises will build towards real-world text analysis tasks in later lessons.\n",
        "* **Encourage Experimentation**: The lesson will include prompts for students to modify the code and observe the results.\n",
        "* **Focus on Practical Skills**: By the end of this lesson, students will be able to write Python code using scikit-learn to perform basic text preprocessing.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Lp7Ewz1wZny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction - What is Text Data and Why Preprocess?\n",
        "\n",
        "* Briefly explain what text data is and why it's important in today's world (examples: online reviews, social media, news articles, documents).\n",
        "\n",
        "* Highlight that raw text is messy and needs cleaning before machine learning models can understand it. Analogy: \"Like cleaning ingredients before cooking!\"\n",
        "\n",
        "* Mention that in this lesson, we'll start with the cleaning part â€“ preprocessing."
      ],
      "metadata": {
        "id": "yqh1sbqKw--Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29zNeWhDvyEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba762df-5f4a-471e-d2e3-b3726572c29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['first' 'is' 'number' 'one' 'second' 'sentence' 'the' 'this' 'three']\n",
            "\n",
            "Document-Term Matrix (Counts):\n",
            "[[1 1 0 0 0 1 1 1 0]\n",
            " [0 1 0 1 1 1 1 1 0]\n",
            " [0 0 1 0 0 1 0 0 1]\n",
            " [1 1 0 0 0 1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"This one is the second sentence.\",\n",
        "    \"Sentence number three.\",\n",
        "    \"Is this the first sentence?\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(corpus) # \"Learn\" vocabulary from the corpus\n",
        "X = vectorizer.transform(corpus) # \"Transform\" text to numbers\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(vectorizer.get_feature_names_out()) # Show the words it learned\n",
        "\n",
        "print(\"\\nDocument-Term Matrix (Counts):\")\n",
        "print(X.toarray()) # Show the numerical representation of the text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code and show the output.\n",
        "* Explain `CountVectorizer` in simple terms: \"*It's like a tool that counts words.*\"\n",
        "\n",
        "* **Vocabulary**: \"Look at 'Vocabulary' - these are all the unique words CountVectorizer found in our text.\"\n",
        "\n",
        "* **Document-Term Matrix**: \"This matrix shows how many times each word appears in each sentence. Each row is a sentence, each column is a word from the vocabulary.\"\n",
        "\n",
        "* Emphasize: \"Machine learning models work with numbers, not text. `CountVectorizer` helps us convert text to numbers!\""
      ],
      "metadata": {
        "id": "ndv22vakxN_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Preprocessing - Lowercasing and Punctuation Removal\n",
        "\n",
        "**Problem**:  Point out issues with the initial tokenization: \"Sentence\" and \"sentence\" are treated as different words. Punctuation is included.  \"This is not ideal for our models.\"\n",
        "\n",
        "**Solution** - *Lowercasing*"
      ],
      "metadata": {
        "id": "clyVFecnxoIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lowercasing"
      ],
      "metadata": {
        "id": "PxtwnxQY0Z3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first Sentence.\",\n",
        "    \"This one is the second sentence.\",\n",
        "    \"Sentence number three.\",\n",
        "    \"Is this the first sentence?\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True) # Add lowercase=True\n",
        "vectorizer.fit(corpus)\n",
        "X = vectorizer.transform(corpus)\n",
        "\n",
        "print(\"Vocabulary (Lowercased):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(\"\\nDocument-Term Matrix (Lowercased):\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "FpWUZYnJxhWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f828bf3-c6d5-4c86-a88a-63afc28f64bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Lowercased):\n",
            "['first' 'is' 'number' 'one' 'second' 'sentence' 'the' 'this' 'three']\n",
            "\n",
            "Document-Term Matrix (Lowercased):\n",
            "[[1 1 0 0 0 1 1 1 0]\n",
            " [0 1 0 1 1 1 1 1 0]\n",
            " [0 0 1 0 0 1 0 0 1]\n",
            " [1 1 0 0 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: \"We added `lowercase=True` inside\n",
        "`CountVectorizer`.\n",
        "\n",
        "Run it again. See?\n",
        "\n",
        "Now 'sentence' and 'Sentence' are treated the same!\""
      ],
      "metadata": {
        "id": "S1HNOskIx4c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove punctuations"
      ],
      "metadata": {
        "id": "vpHKfMKF0SiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"This one is the second sentence.\",\n",
        "    \"Sentence number three!\", # Added punctuation\n",
        "    \"Is this the first sentence?\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True, token_pattern=r'[a-zA-Z]+') # Added token_pattern\n",
        "vectorizer.fit(corpus)\n",
        "X = vectorizer.transform(corpus)\n",
        "\n",
        "print(\"Vocabulary (No Punctuation, Lowercased):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(\"\\nDocument-Term Matrix (No Punctuation, Lowercased):\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "Mejk0jei0Bxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31cd3ef4-874b-401e-bc73-1e0a74f942a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (No Punctuation, Lowercased):\n",
            "['first' 'is' 'number' 'one' 'second' 'sentence' 'the' 'this' 'three']\n",
            "\n",
            "Document-Term Matrix (No Punctuation, Lowercased):\n",
            "[[1 1 0 0 0 1 1 1 0]\n",
            " [0 1 0 1 1 1 1 1 0]\n",
            " [0 0 1 0 0 1 0 0 1]\n",
            " [1 1 0 0 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Explanation**: \"We added token_pattern=r'[a-zA-Z]+'. This tells CountVectorizer to only keep words made of letters. Punctuation is gone!\"\n",
        "* Explain token_pattern briefly as \"a rule for what counts as a word.\" (No need to go deep into regex at this stage, just the practical effect.)"
      ],
      "metadata": {
        "id": "wB6EFZ5C0kQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stop Word Removal - Using Built-in Stop Words in `CountVectorizer`"
      ],
      "metadata": {
        "id": "gvl9qfXD0w02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**:  Point out common words like \"is,\" \"the,\" \"this,\" \"one\" are very frequent but might not be very informative for analysis. These are called 'stop words'.\n",
        "\n",
        "**Solution** - Stop Word Removal"
      ],
      "metadata": {
        "id": "OKYfcuQE02Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"This one is the second sentence.\",\n",
        "    \"Sentence number three!\",\n",
        "    \"Is this the first sentence?\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    token_pattern=r'[a-zA-Z]+',\n",
        "    stop_words='english') # Added stop_words='english'\n",
        "vectorizer.fit(corpus)\n",
        "X = vectorizer.transform(corpus)\n",
        "\n",
        "print(\"Vocabulary (No Punctuation, Lowercased, Stop Words Removed):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(\"\\nDocument-Term Matrix (No Punctuation, Lowercased, Stop Words Removed):\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "Bqgn0iHe0sjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0962550f-06ec-4313-9e52-b29b3537d5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (No Punctuation, Lowercased, Stop Words Removed):\n",
            "['number' 'second' 'sentence']\n",
            "\n",
            "Document-Term Matrix (No Punctuation, Lowercased, Stop Words Removed):\n",
            "[[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We added stop_words='english'. CountVectorizer now automatically removes common English words. Run it and see how the vocabulary is smaller and common words like 'is', 'the' are gone!\n",
        "\n",
        "`stop_words='english'` uses a predefined list. We can customize this later if needed."
      ],
      "metadata": {
        "id": "Duw8tygh1DTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentation"
      ],
      "metadata": {
        "id": "nawetuUS1Hlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment # 1**: Change the corpus to different sentences. What happens to the vocabulary and the matrix?"
      ],
      "metadata": {
        "id": "Z7497xQC1LW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "cmLP2LV21TlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment #2**:Remove `lowercase=True`. What changes?"
      ],
      "metadata": {
        "id": "po6PEvbo1WIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "xE1cMhmg1VY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment #3**: Remove `stop_words='english'`. What comes back in the vocabulary?"
      ],
      "metadata": {
        "id": "ElN-p6Wf1hD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "v4NDKOLA1o52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment #4**: Try different token_pattern values."
      ],
      "metadata": {
        "id": "Jn8ZkNCJ1qsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "6JS0xEOb2JD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "* In this lesson, we learned how to take raw text and use `CountVectorizer` to convert it into numbers that machine learning models can use.\n",
        "\n",
        "* We covered basic preprocessing steps: lowercasing, punctuation removal, and stop word removal, all using `CountVectorizer`.\n",
        "\n",
        "* This is just the beginning! In the next lessons, we'll use these numerical representations to build actual text classification models!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w8ZIjmlq1cow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next time, we'll learn about TF-IDF and then start building models like Logistic Regression!"
      ],
      "metadata": {
        "id": "jw5luIb62WAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "* Text data needs preprocessing to be used in machine learning.\n",
        "* `CountVectorizer` in scikit-learn is a powerful tool for tokenization and basic preprocessing.\n",
        "* We can control preprocessing steps like lowercasing, punctuation removal, and stop word removal within `CountVectorizer`.\n",
        "* The output of `CountVectorizer` is a numerical matrix that represents text data."
      ],
      "metadata": {
        "id": "8enRaZa92YaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "\n",
        "`Scikit-learn` documentation on `CountVectorizer`: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ],
      "metadata": {
        "id": "FE8sAUfl2nyK"
      }
    }
  ]
}