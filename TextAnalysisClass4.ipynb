{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTcIPe+2tovEQfHK8lp5Of",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfindiamooc/mlp/blob/feature/TextAnalysisClass4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson #4: Unsupervised Text Clustering with K-Means\n",
        "\n",
        "Welcome to a lesson on **Text Clustering**!  So far, we've focused on **supervised text classification**, where we have labeled data to train models to predict categories. Now, we'll explore **unsupervised learning** with text clustering, where we aim to discover hidden structures and group similar documents **without any pre-defined labels**.\n",
        "\n",
        "In this lesson, you will:\n",
        "\n",
        "*   Understand **what text clustering is and when to use it**.\n",
        "*   Learn about the **K-Means clustering algorithm**.\n",
        "*   Build **K-Means clustering pipelines** for text data.\n",
        "*   Explore **text vectorization** techniques for clustering.\n",
        "*   Learn how to **evaluate** text clustering using the **Silhouette Score**.\n",
        "*   **Inspect and interpret** text clusters by examining top terms.\n",
        "*   Experiment with **choosing the optimal number of clusters (K)**.\n",
        "\n",
        "Let's start by building a basic K-Means clustering pipeline for text!"
      ],
      "metadata": {
        "id": "j1nEBu5uOLcB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPje0D2sTp9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK_fiBs0OHTA"
      },
      "outputs": [],
      "source": [
        "# Code Cell 1: Basic K-Means Clustering Pipeline Code\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.cluster import KMeans # Import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score # Import silhouette_score\n",
        "\n",
        "# 1. Load Dataset (using a subset for clustering demonstration - no categories needed)\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics']) # More categories for clustering\n",
        "X = newsgroups.data # Only data, no labels (unsupervised)\n",
        "target_names = newsgroups.target_names\n",
        "\n",
        "# 2. Vectorize Text Data (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X) # Fit and transform for clustering\n",
        "\n",
        "# 3. Apply K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # Set n_clusters = number of categories for demonstration\n",
        "clusters = kmeans.fit_predict(X_tfidf) # Fit and predict clusters\n",
        "\n",
        "# 4. Evaluate Clustering (Silhouette Score)\n",
        "silhouette_avg = silhouette_score(X_tfidf, clusters)\n",
        "print(f\"Silhouette Score for K-Means Clustering: {silhouette_avg:.4f}\")\n",
        "\n",
        "# 5. (Optional) Print cluster sizes\n",
        "import pandas as pd\n",
        "cluster_series = pd.Series(clusters)\n",
        "print(\"\\nCluster Sizes:\")\n",
        "print(cluster_series.value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means Clustering Pipeline - First Steps\n",
        "\n",
        "This code sets up a basic K-Means clustering pipeline for text. Let's break down the steps:\n",
        "\n",
        "1.  **Dataset Loading (Unlabeled Data):**\n",
        "    *   We load the 20 Newsgroups dataset, but this time, we are using it for **unsupervised learning**. We are *not* using the `target` labels for clustering itself.\n",
        "    *   We've selected a few more categories (`['alt.atheism', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics']`) to make the clustering task a bit more interesting.\n",
        "    *   We extract only `newsgroups.data` (the text documents) as `X`. We don't need `y` (labels) for clustering.\n",
        "\n",
        "2.  **Text Vectorization (TF-IDF):**\n",
        "    *   We use `TfidfVectorizer` to convert the text documents into a numerical matrix representation (TF-IDF matrix). This is essential because clustering algorithms like K-Means work with numerical data.\n",
        "\n",
        "3.  **K-Means Clustering:**\n",
        "    *   **`KMeans(n_clusters=4, ...)`**: We initialize the `KMeans` algorithm.\n",
        "        *   **`n_clusters=4`**:  We are telling K-Means to find **4 clusters**.  Since we selected 4 categories from 20 Newsgroups, we are setting `n_clusters=4` for demonstration purposes to see if K-Means can roughly recover these topics. In a real-world unsupervised scenario, you often won't know the \"true\" number of clusters beforehand. We'll discuss how to choose `n_clusters` later.\n",
        "        *   `random_state=42`: For reproducibility.\n",
        "        *   `n_init=10`:  K-Means starts with random initial cluster centers. `n_init=10` means it will run the algorithm 10 times with different random initializations and choose the best result (in terms of inertia).\n",
        "\n",
        "    *   **`clusters = kmeans.fit_predict(X_tfidf)`**: We train the K-Means model on the TF-IDF vectorized data (`X_tfidf`) and get cluster assignments for each document. `clusters` will be an array where each element is the cluster index (0, 1, 2, or 3 in this case) assigned to the corresponding document.\n",
        "\n",
        "4.  **Evaluate Clustering (Silhouette Score):**\n",
        "    *   **`silhouette_score(X_tfidf, clusters)`**: We use the **Silhouette Score** to evaluate the quality of the clustering.\n",
        "        *   **Silhouette Score:** Measures how well each document is clustered with documents in its own cluster, compared to documents in other clusters.\n",
        "        *   Silhouette Score ranges from -1 to +1:\n",
        "            *   **+1:** Best value. Indicates clusters are well-separated and documents are well-clustered within their own cluster.\n",
        "            *   **0:**  Clusters are overlapping, or documents are on cluster boundaries.\n",
        "            *   **-1:** Worst value. Indicates documents might be better clustered in a *different* cluster.\n",
        "        *   For text clustering, we generally aim for Silhouette Scores that are positive and as close to +1 as possible, but in practice, scores are often lower.\n",
        "\n",
        "5.  **(Optional) Print Cluster Sizes:** We use `pandas` to count the number of documents in each cluster to get an idea of cluster distribution.\n",
        "\n",
        "Run this code to see the Silhouette Score and cluster sizes for the basic K-Means pipeline.\n",
        "\n",
        "Now, let's understand more about what text clustering is and how K-Means works."
      ],
      "metadata": {
        "id": "97LnlTHNOQT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Text Clustering? - Finding Structure in Unlabeled Text\n",
        "\n",
        "**Text clustering** is an **unsupervised learning** task that aims to group similar text documents together into clusters, without using any pre-defined categories or labels.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "*   **Unsupervised Learning:**  Unlike classification (supervised), clustering works with **unlabeled data**. We don't have pre-assigned categories for documents. The goal is to discover groupings based on the content of the documents themselves.\n",
        "\n",
        "*   **Document Similarity:** Clustering relies on the concept of **document similarity**.  Documents within the same cluster should be more similar to each other than to documents in other clusters. Similarity is typically measured using vector representations of text (like TF-IDF vectors) and distance metrics (like cosine distance or Euclidean distance).\n",
        "\n",
        "*   **Discovering Hidden Structure:** Text clustering is used to **discover hidden thematic structures** or topics within a collection of documents.  It can help you:\n",
        "    *   **Automatically group documents by topic.**\n",
        "    *   **Explore and understand the main themes** present in a text corpus.\n",
        "    *   **Organize large collections of text data.**\n",
        "    *   **Identify sub-groups within a user base based on their text data (e.g., customer reviews, social media posts).**\n",
        "\n",
        "*   **Contrast with Text Classification:**\n",
        "    *   **Text Classification (Supervised):**  Predicts pre-defined categories for documents based on labeled training data. You know the categories in advance.\n",
        "    *   **Text Clustering (Unsupervised):**  Discovers groupings of documents automatically, without pre-defined categories. You don't know the topics beforehand; the algorithm finds them.\n",
        "\n",
        "**Common Text Clustering Algorithms:**\n",
        "\n",
        "*   **K-Means:**  A popular centroid-based algorithm (we'll focus on this).\n",
        "*   **Hierarchical Clustering:** Builds a hierarchy of clusters (agglomerative or divisive).\n",
        "*   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**  Finds clusters based on density of data points.\n",
        "*   **Topic Modeling (Latent Dirichlet Allocation - LDA, Non-negative Matrix Factorization - NMF):**  Related to clustering, but focuses on discovering latent topics and topic distributions within documents. We might explore topic modeling in a later lesson.\n",
        "\n",
        "**Why use Text Clustering?**\n",
        "\n",
        "*   **Exploratory Data Analysis:**  Great for understanding the structure of a new text dataset when you don't have labels.\n",
        "*   **Organization and Summarization:**  Helps organize and summarize large text collections by grouping similar documents.\n",
        "*   **Feature Engineering:**  Cluster assignments can sometimes be used as features in supervised learning tasks.\n",
        "*   **No Labeled Data Required:**  A major advantage is that you don't need manually labeled data, which can be expensive and time-consuming to obtain.\n",
        "\n",
        "In the next text cell, we'll focus on the K-Means algorithm itself."
      ],
      "metadata": {
        "id": "qo68QGRNOVeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means Clustering Algorithm - Step-by-Step\n",
        "\n",
        "**K-Means** is a widely used **centroid-based** clustering algorithm. Here's how it works:\n",
        "\n",
        "1.  **Initialization: Choose K and Initial Centroids:**\n",
        "    *   You need to specify **`K`**, the **number of clusters** you want to find. This is a hyperparameter you need to decide (we'll discuss how to choose K later).\n",
        "    *   **Initial centroids** are randomly chosen. A centroid is the center point of a cluster.  For text data vectorized into a TF-IDF matrix, a centroid is a vector in the same feature space.\n",
        "\n",
        "2.  **Assignment Step: Assign Documents to the Nearest Centroid:**\n",
        "    *   For each document, calculate the **distance** (e.g., cosine distance, Euclidean distance) between the document's vector representation and each of the **K centroids**.\n",
        "    *   Assign each document to the cluster whose centroid is **closest** to it.\n",
        "\n",
        "3.  **Update Step: Recalculate Centroids:**\n",
        "    *   For each cluster, recalculate the **centroid** by taking the **mean** of all the document vectors assigned to that cluster.  The new centroid becomes the average vector of all documents in the cluster.\n",
        "\n",
        "4.  **Iteration:**\n",
        "    *   Repeat steps 2 and 3 (Assignment and Update) until **convergence**.\n",
        "    *   **Convergence** occurs when the cluster assignments no longer change significantly, or when a maximum number of iterations is reached.\n",
        "\n",
        "5.  **Final Clusters:**  Once converged, you have your final clusters, and each document is assigned to one of the K clusters.\n",
        "\n",
        "**Visual Intuition:**\n",
        "\n",
        "Imagine you have points scattered on a 2D plane (think of document vectors in a high-dimensional space). K-Means tries to find K cluster centers (centroids) and group the points around these centers, so that points within each group are close to each other and far from points in other groups.\n",
        "\n",
        "**Important Considerations for K-Means:**\n",
        "\n",
        "*   **Choosing K:** Selecting the correct number of clusters (`K`) is crucial and often not straightforward in unsupervised learning. We'll explore methods like the Elbow method and Silhouette Score to help choose K.\n",
        "*   **Initialization Sensitivity:** K-Means is sensitive to the initial random placement of centroids. Running K-Means multiple times with different random initializations (controlled by `n_init` parameter in scikit-learn) and choosing the best result helps mitigate this issue.\n",
        "*   **Distance Metric:** The choice of distance metric (e.g., cosine, Euclidean) can affect clustering results. Cosine distance is often preferred for text data, especially with TF-IDF, as it focuses on the angle between vectors (topic similarity) rather than magnitude.  However, scikit-learn's `KMeans` in `sklearn.cluster` primarily uses Euclidean distance. We'll use TF-IDF and Euclidean distance in this lesson for simplicity, but be aware of cosine distance as an alternative.\n",
        "*   **Spherical Clusters:** K-Means assumes clusters are somewhat spherical and equally sized. It might not perform well if clusters have complex shapes or varying densities.\n",
        "\n",
        "Let's now experiment with different vectorizers in our K-Means clustering pipeline."
      ],
      "metadata": {
        "id": "dC_nbCXQOYL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 2: K-Means with different vectorizers (CountVectorizer, TF-IDF)\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# 1. Load Dataset (Same as before)\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics'])\n",
        "X = newsgroups.data\n",
        "target_names = newsgroups.target_names\n",
        "\n",
        "# 2. Pipelines with different vectorizers\n",
        "# Pipeline with CountVectorizer (BoW)\n",
        "kmeans_pipeline_bow = Pipeline([\n",
        "    ('bow', CountVectorizer(stop_words='english', max_features=5000)), # CountVectorizer\n",
        "    ('kmeans', KMeans(n_clusters=4, random_state=42, n_init=10))\n",
        "])\n",
        "\n",
        "# Pipeline with TfidfVectorizer (TF-IDF) - (Same as before for comparison)\n",
        "kmeans_pipeline_tfidf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)), # TfidfVectorizer\n",
        "    ('kmeans', KMeans(n_clusters=4, random_state=42, n_init=10))\n",
        "])\n",
        "\n",
        "# 3. Fit and Predict Clusters using Pipelines\n",
        "clusters_bow = kmeans_pipeline_bow.fit_predict(X) # Fit and predict in one step for pipeline\n",
        "clusters_tfidf = kmeans_pipeline_tfidf.fit_predict(X)\n",
        "\n",
        "# 4. Evaluate Clustering - Silhouette Score (Compare BoW and TF-IDF)\n",
        "silhouette_avg_bow = silhouette_score(kmeans_pipeline_bow.transform(X), clusters_bow) # Use transform to get vectorized data for silhouette_score\n",
        "silhouette_avg_tfidf = silhouette_score(kmeans_pipeline_tfidf.transform(X), clusters_tfidf)\n",
        "\n",
        "print(f\"Silhouette Score for K-Means with CountVectorizer (BoW): {silhouette_avg_bow:.4f}\")\n",
        "print(f\"Silhouette Score for K-Means with TfidfVectorizer (TF-IDF): {silhouette_avg_tfidf:.4f}\")"
      ],
      "metadata": {
        "id": "4eB2YJsnObPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Vectorizers with K-Means\n",
        "\n",
        "In this code, we compare K-Means clustering performance using two different text vectorizers:\n",
        "\n",
        "*   **`kmeans_pipeline_bow`**: Uses `CountVectorizer` (Bag of Words).\n",
        "*   **`kmeans_pipeline_tfidf`**: Uses `TfidfVectorizer` (TF-IDF).\n",
        "\n",
        "Run the code and compare the Silhouette Scores for both pipelines.\n",
        "\n",
        "**Questions to consider:**\n",
        "\n",
        "*   Does using TF-IDF vectorization lead to a better Silhouette Score compared to using Bag of Words (CountVectorizer) for K-Means clustering?\n",
        "*   Is the difference in Silhouette Scores significant?\n",
        "*   Based on these scores, which vectorization method seems to produce better-defined clusters for K-Means?\n",
        "\n",
        "Generally, TF-IDF is often preferred for text clustering as it downweights common words and emphasizes words that are more distinctive to specific documents, which can help K-Means find better clusters.\n",
        "\n",
        "Now, let's inspect the clusters to understand the topics K-Means has discovered."
      ],
      "metadata": {
        "id": "ZG6TtGV_OZKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: Inspecting K-Means Clusters - Top Terms\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load Dataset & Vectorize (TF-IDF Pipeline - for inspection)\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics'])\n",
        "X = newsgroups.data\n",
        "target_names = newsgroups.target_names\n",
        "\n",
        "kmeans_pipeline_tfidf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
        "    ('kmeans', KMeans(n_clusters=4, random_state=42, n_init=10))\n",
        "])\n",
        "\n",
        "# 2. Fit Pipeline and Get Clusters\n",
        "clusters_tfidf = kmeans_pipeline_tfidf.fit_predict(X)\n",
        "\n",
        "# 3. Get Vectorizer and KMeans model from pipeline\n",
        "tfidf_vectorizer = kmeans_pipeline_tfidf.named_steps['tfidf']\n",
        "kmeans_model = kmeans_pipeline_tfidf.named_steps['kmeans']\n",
        "\n",
        "# 4. Get Feature Names (Words) and Cluster Centers\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out() # Get word vocabulary\n",
        "cluster_centers = kmeans_model.cluster_centers_ # Get cluster centroids (vectors)\n",
        "\n",
        "# 5. Function to get top terms per cluster\n",
        "def get_top_terms(cluster_index, top_n=15):\n",
        "    centroid = cluster_centers[cluster_index] # Get centroid for cluster\n",
        "    top_term_indices = centroid.argsort()[-top_n:][::-1] # Get indices of top terms (words) in centroid\n",
        "    top_terms = feature_names[top_term_indices] # Get actual words\n",
        "    return top_terms\n",
        "\n",
        "# 6. Print Top Terms for each cluster\n",
        "print(\"Top terms per cluster:\")\n",
        "for i in range(4): # Assuming n_clusters=4\n",
        "    top_terms = get_top_terms(i)\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "    for term in top_terms:\n",
        "        print(f\"- {term}\")\n",
        "\n",
        "# 7. (Optional) Print some example documents from each cluster (for qualitative inspection)\n",
        "print(\"\\nExample documents from each cluster (first 2 from each):\")\n",
        "for i in range(4):\n",
        "    print(f\"\\nCluster {i} - Example Documents:\")\n",
        "    cluster_docs_indices = np.where(clusters_tfidf == i)[0] # Indices of docs in cluster i\n",
        "    for doc_index in cluster_docs_indices[:2]: # First 2 documents\n",
        "        print(f\"Doc index: {doc_index}:\")\n",
        "        print(X[doc_index][:200] + \"...\") # Print first 200 chars of doc\n",
        "        print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "K_MXRsvZOe0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of K-Means Cluster Inspection - Top Terms\n",
        "\n",
        "Let's understand the cluster inspection code:\n",
        "\n",
        "*   **Steps 1-3:**  We set up and fit the K-Means pipeline with TF-IDF, similar to before. We also extract the trained `TfidfVectorizer` and `KMeans` model from the pipeline.\n",
        "\n",
        "*   **Step 4: Get Feature Names and Cluster Centers:**\n",
        "    *   **`feature_names = tfidf_vectorizer.get_feature_names_out()`**: Gets the vocabulary (words) from the `TfidfVectorizer`.\n",
        "    *   **`cluster_centers = kmeans_model.cluster_centers_`**:  This is important! It retrieves the **cluster centroids** from the trained K-Means model.\n",
        "        *   **Cluster Centroids:** In K-Means, each cluster is represented by a centroid, which is the mean vector of all documents assigned to that cluster.\n",
        "        *   For TF-IDF vectorized text, a centroid is a vector in the same TF-IDF feature space.  The values in the centroid vector represent the \"average\" TF-IDF weight of each word in the cluster.\n",
        "\n",
        "*   **Step 5: `get_top_terms(cluster_index, top_n=15)` Function:**\n",
        "    *   This function takes a `cluster_index` (0, 1, 2, 3 in our case) and `top_n` (number of top terms to retrieve).\n",
        "    *   **`centroid = cluster_centers[cluster_index]`**: Gets the centroid vector for the specified cluster.\n",
        "    *   **`top_term_indices = centroid.argsort()[-top_n:][::-1]`**:  Finds the indices of the **top `top_n` terms** in the centroid vector.\n",
        "        *   `centroid.argsort()`: Returns indices that would sort the centroid vector in ascending order.\n",
        "        *   `[-top_n:]`: Selects the indices of the `top_n` largest values (highest TF-IDF weights) in the centroid.\n",
        "        *   `[::-1]`: Reverses the order to get indices in descending order (highest to lowest weights).\n",
        "    *   **`top_terms = feature_names[top_term_indices]`**:  Uses the `top_term_indices` to retrieve the actual **words** (feature names) from the vocabulary.\n",
        "\n",
        "*   **Step 6: Print Top Terms for each cluster:**  We iterate through each cluster (0 to 3) and call `get_top_terms()` to get the top words for each cluster and print them.\n",
        "\n",
        "*   **Step 7: (Optional) Print Example Documents:**  This part is for qualitative inspection. It prints the first 200 characters of the first 2 documents assigned to each cluster to give you a sense of what kind of documents are in each cluster.\n",
        "\n",
        "Run this code and examine the \"Top terms per cluster\" output.\n",
        "\n",
        "**Interpreting Top Terms:**\n",
        "\n",
        "*   Examine the top words for each cluster. Do they seem to represent coherent topics?\n",
        "*   Do the top terms for each cluster relate to the original categories we used (`alt.atheism`, `soc.religion.christian`, `talk.politics.mideast`, `comp.graphics`)?  Remember, clustering is unsupervised, so it won't perfectly match the original categories, but you should see some thematic overlap if clustering is working reasonably well.\n",
        "*   Look at the example documents to get a qualitative sense of the cluster content.\n",
        "\n",
        "Now, let's discuss how to choose the number of clusters, `K`."
      ],
      "metadata": {
        "id": "x_FN_1O4OhVb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaS4vbVBOj-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the Number of Clusters (K) - Elbow Method and Silhouette Score\n",
        "\n",
        "Choosing the right number of clusters (`K`) is a critical step in K-Means clustering (and in clustering in general).  In many real-world scenarios, you won't know the \"true\" number of clusters beforehand.  Here are two common methods to help you choose K:\n",
        "\n",
        "**1. Elbow Method (using Inertia):**\n",
        "\n",
        "*   **Inertia:** In K-Means, **inertia** is the sum of squared distances of samples to their closest cluster center.  It represents the within-cluster sum of squares. Lower inertia is better, meaning clusters are more compact.\n",
        "*   **Elbow Plot:**\n",
        "    *   Run K-Means for a range of possible `K` values (e.g., from 2 to 10).\n",
        "    *   For each `K`, calculate the **inertia**.\n",
        "    *   Plot **Inertia vs. K**.\n",
        "    *   Look for an \"elbow\" in the plot. The \"elbow\" point is often considered a good indication of a reasonable `K`.  The idea is that inertia decreases as K increases, but the rate of decrease slows down after the \"elbow.\"\n",
        "\n",
        "**2. Silhouette Score Method:**\n",
        "\n",
        "*   We've already used the Silhouette Score to evaluate a clustering for a fixed `K`.\n",
        "*   **Silhouette Score Plot:**\n",
        "    *   Run K-Means for a range of `K` values (e.g., from 2 to 10).\n",
        "    *   For each `K`, calculate the **average Silhouette Score** for all documents.\n",
        "    *   Plot **Silhouette Score vs. K**.\n",
        "    *   Look for the `K` that maximizes the Silhouette Score.  A higher Silhouette Score indicates better-defined clusters.\n",
        "\n"
      ],
      "metadata": {
        "id": "VN5UPXowOmZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "**Code Example - Combining Elbow Method and Silhouette Score:**\n",
        "```python\n",
        "# Code Cell 4: Choosing K - Elbow Method and Silhouette Score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load Dataset & Vectorize (TF-IDF Pipeline)\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'soc.religion.christian', 'talk.politics.mideast', 'comp.graphics'])\n",
        "X = newsgroups.data\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# 2. Range of K values to try\n",
        "k_range = range(2, 11) # Try K from 2 to 10\n",
        "\n",
        "# 3. Lists to store inertia and silhouette scores\n",
        "inertia_values = []\n",
        "silhouette_scores = []\n",
        "\n",
        "# 4. Loop through different K values and run K-Means\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(X_tfidf)\n",
        "    inertia_values.append(kmeans.inertia_) # Store inertia\n",
        "    silhouette_avg = silhouette_score(X_tfidf, clusters) # Calculate Silhouette Score\n",
        "    silhouette_scores.append(silhouette_avg) # Store Silhouette Score\n",
        "    print(f\"For K={k}, Silhouette Score: {silhouette_avg:.4f}\") # Print Silhouette Score for each K\n",
        "\n",
        "# 5. Elbow Plot (Inertia vs. K)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_range, inertia_values, marker='o')\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method for Optimal K\")\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "\n",
        "# 6. Silhouette Score Plot (Silhouette Score vs. K)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_range, silhouette_scores, marker='o', color='green')\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Score for Optimal K\")\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N-D-q56uOr0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this code to generate the Elbow Plot and Silhouette Score plot.\n",
        "\n",
        "Interpreting the Plots:\n",
        "\n",
        "* **Elbow Plot**: Look for an \"elbow\" point in the Inertia plot. Where does the decrease in inertia start to become less steep? This point might suggest a reasonable K.\n",
        "* **Silhouette Score Plot**: Look for the K value that corresponds to the highest Silhouette Score.\n",
        "\n"
      ],
      "metadata": {
        "id": "_pRYdS99OtjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Notes on Choosing K**:\n",
        "\n",
        "* **No Single \"Best\" K**: In many unsupervised clustering tasks, there isn't a single definitively \"correct\" number of clusters. The \"best\" K often depends on your goals and how you want to interpret the clusters.\n",
        "* **Domain Knowledge**: Domain knowledge about your data can be helpful in guiding your choice of K. For example, if you are clustering news articles and you know there are roughly 5-7 major topics covered, you might try K values in that range.\n",
        "* **Iterative Process**: Choosing K is often an iterative process. You might try different K values, inspect the resulting clusters (top terms, example documents), evaluate using metrics like Silhouette Score and Inertia, and then refine your choice of K based on these insights.\n",
        "* **Other Evaluation Metrics**: Besides Silhouette Score and Inertia, other clustering evaluation metrics exist (e.g., [Davies-Bouldin Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html). You can explore these as well.\n",
        "\n",
        "Now, let's move on to experimentation prompts to further explore text clustering with K-Means."
      ],
      "metadata": {
        "id": "wwsffSl1P0a7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llB_uTCKO27x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimentation Prompts - K-Means Text Clustering Deep Dive\n",
        "\n",
        "Time to experiment with K-Means text clustering! Try these:\n",
        "\n",
        "1.  **Datasets and K-Means Clustering:**\n",
        "    *   Change the `categories` in `fetch_20newsgroups` to different sets of categories (e.g., more categories, different combinations).\n",
        "    *   For each dataset:\n",
        "        *   Run the Elbow method and Silhouette Score analysis to help choose a suitable `K`.\n",
        "        *   Cluster the data using K-Means with your chosen `K`.\n",
        "        *   Inspect the clusters (top terms, example documents).\n",
        "        *   How do the clusters and evaluation metrics change with different datasets? Does K-Means find meaningful clusters for different topic sets?\n",
        "\n",
        "2.  **Vectorizers and K-Means - Deeper Exploration:**\n",
        "    *   Experiment with different vectorizers and vectorizer parameters in your K-Means pipeline:\n",
        "        *   **`CountVectorizer` vs. `TfidfVectorizer`:** Compare them systematically across different datasets and K values.\n",
        "        *   **Vary `ngram_range`, `max_df`, `min_df`, `max_features` in `TfidfVectorizer` and `CountVectorizer`.** How do these vectorizer parameters affect clustering quality (Silhouette Score, cluster interpretability)?\n",
        "\n",
        "3.  **Distance Metrics in K-Means (Advanced - Optional):**\n",
        "    *   By default, scikit-learn's `KMeans` uses Euclidean distance.  While cosine distance is often preferred for text similarity, it's not directly available as a built-in metric in `sklearn.cluster.KMeans`.\n",
        "    *   **If you want to experiment with cosine distance:**\n",
        "        *   You would need to pre-normalize your TF-IDF vectors (e.g., using `sklearn.preprocessing.normalize` with `norm='l2'`) so that Euclidean distance approximates cosine distance.  Or, you might need to explore other K-Means implementations that directly support cosine distance (which might be more advanced).\n",
        "        *   Does using cosine distance (or approximating it with normalized vectors and Euclidean distance) improve clustering results compared to standard Euclidean distance on raw TF-IDF vectors?\n",
        "\n",
        "4.  **Initialization Methods for K-Means (Advanced - Optional):**\n",
        "    *   Experiment with different initialization methods for K-Means using the `init` parameter:\n",
        "        *   `init='k-means++'` (default - smart initialization that often leads to better results and faster convergence).\n",
        "        *   `init='random'` (random initialization).\n",
        "        *   How does the initialization method affect clustering performance (Silhouette Score, inertia, convergence speed)?\n",
        "\n",
        "Think about these questions as you experiment:\n",
        "\n",
        "*   How robust is K-Means clustering for text data? Does it consistently find meaningful clusters?\n",
        "*   What is the impact of vectorization choices on clustering quality?\n",
        "*   How sensitive is K-Means to the choice of `K`?\n",
        "*   When is K-Means a good choice for text clustering, and when might you consider other clustering algorithms or topic modeling techniques?\n",
        "\n",
        "After your experiments, read the summary and key takeaways for this lesson."
      ],
      "metadata": {
        "id": "XFZw5XBRO4a2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zU1oNg6O5b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary and Next Steps - Unveiling Text Structure with K-Means\n",
        "\n",
        "Excellent work exploring text clustering with K-Means! In this lesson, you've:\n",
        "\n",
        "*   Understood **text clustering** as an unsupervised learning task for grouping similar documents.\n",
        "*   Learned about the **K-Means clustering algorithm** and its iterative process.\n",
        "*   Built **K-Means clustering pipelines** for text data using scikit-learn.\n",
        "*   Experimented with **text vectorization** (TF-IDF, CountVectorizer) for clustering.\n",
        "*   Used the **Silhouette Score** to evaluate clustering quality.\n",
        "*   Learned how to **inspect clusters** by examining top terms and example documents.\n",
        "*   Explored methods for **choosing the number of clusters (K)**, including the Elbow method and Silhouette Score analysis.\n",
        "\n",
        "**Key Takeaways for K-Means Text Clustering:**\n",
        "\n",
        "*   Text clustering is a powerful technique for **discovering hidden structure and organizing unlabeled text data**.\n",
        "*   **K-Means is a widely used and relatively simple clustering algorithm**, but it can be effective for text clustering.\n",
        "*   **Text vectorization (like TF-IDF) is essential** to represent text documents numerically for K-Means.\n",
        "*   **Choosing the number of clusters (K) is a crucial step**, and methods like the Elbow method and Silhouette Score can help guide this choice.\n",
        "*   **Inspecting clusters (top terms, example documents) is important** to understand the topics discovered by K-Means and evaluate the quality of the clustering qualitatively.\n",
        "*   K-Means is a good starting point for text clustering, but be aware of its limitations (sensitivity to initialization, assumptions about cluster shape, need to pre-specify K).\n",
        "\n",
        "* There exists more advanced techniques for text analysis and unsupervised learning, which are beyong the scope of this course:\n",
        "   *   **Topic Modeling (LDA, NMF)** - to discover latent topics in more detail.\n",
        "   *   **Word Embeddings** (Word2Vec, GloVe, FastText) for richer text representations that can be used for both clustering and classification\n",
        "   *   Potentially, other clustering algorithms beyond K-Means (e.g., Hierarchical Clustering, DBSCAN).\n",
        "\n",
        "You are now equipped with valuable unsupervised learning skills for text data! Keep experimenting and exploring different datasets and techniques!"
      ],
      "metadata": {
        "id": "lOeu1C4NO7Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways for Lesson #3d (K-Means Text Clustering Specific):\n",
        "\n",
        "*   **Text Clustering is unsupervised learning to group similar documents without labels.**\n",
        "*   **K-Means is a centroid-based algorithm that iteratively assigns documents to clusters and updates centroids.**\n",
        "*   **Vectorize text (e.g., TF-IDF) before applying K-Means.**\n",
        "*   **Silhouette Score evaluates clustering quality.**\n",
        "*   **Inspect clusters by examining top terms and example documents.**\n",
        "*   **Choose K using Elbow method and Silhouette Score analysis (iterative process).**\n",
        "*   **K-Means is a useful baseline for text clustering, but has limitations.**"
      ],
      "metadata": {
        "id": "5b6_UYgUO7yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resources for Lesson #3d (K-Means Text Clustering Specific):\n",
        "\n",
        "*   **Scikit-learn documentation on `KMeans`:** [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
        "*   **Scikit-learn documentation on `silhouette_score`:** [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)"
      ],
      "metadata": {
        "id": "-Mn4OqQkO_Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Notes - K-Means Text Clustering Specific Considerations:\n",
        "\n",
        "*   **Pre-processing:** Text pre-processing steps (lowercase, punctuation removal, stop word removal, stemming/lemmatization) are important for text clustering, just as they are for classification. Experiment with different pre-processing strategies.\n",
        "\n",
        "*   **Feature Scaling (for Euclidean Distance):** If you are using Euclidean distance with K-Means on TF-IDF vectors, consider normalizing the TF-IDF vectors (e.g., using L2 normalization) as TF-IDF can have varying magnitudes. Normalization can help ensure that K-Means is less sensitive to document length and focuses more on term frequencies.\n",
        "\n",
        "*   **High Dimensionality:** Text data is often high-dimensional (many words/features).  Dimensionality reduction techniques (like Principal Component Analysis - PCA or Non-negative Matrix Factorization - NMF) can sometimes be applied *before* K-Means to reduce dimensionality and potentially improve clustering, especially for very large vocabularies. However, for moderately sized vocabularies (like max_features=5000 in our examples), dimensionality reduction might not always be necessary or beneficial.\n",
        "\n",
        "*   **Cluster Size Imbalance:** K-Means can sometimes produce clusters of very different sizes. This might be acceptable depending on your data, but if you want more balanced clusters, you might explore other clustering algorithms or techniques to address cluster imbalance.\n",
        "\n",
        "*   **Beyond K-Means:**  Remember that K-Means is just one clustering algorithm. For more complex text clustering tasks, consider exploring other algorithms like Hierarchical Clustering, DBSCAN, or topic modeling techniques like LDA and NMF, which might be better suited for discovering more nuanced thematic structures in text."
      ],
      "metadata": {
        "id": "AhpHAzNNPCLw"
      }
    }
  ]
}