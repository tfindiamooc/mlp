{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSuHOC0PYmeLajK3vZBtnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfindiamooc/mlp/blob/main/TextAnalysisClass3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson #3d: Streamlined Feature Engineering: Pipelines and Transformations for Combined Data\n",
        "\n",
        "Welcome back! In this lesson, we'll refine our approach to combining text and numerical data by focusing on **scikit-learn Pipelines** and **Feature Transformations**. Pipelines are essential for building robust and organized machine learning workflows, and feature transformations are key to preparing different data types for effective modeling.\n",
        "\n",
        "In this lesson, you will:\n",
        "\n",
        "*   Deepen your understanding of **scikit-learn Pipelines** for end-to-end workflows.\n",
        "*   Master the use of **Feature Transformers** for numerical and text data within pipelines.\n",
        "*   Learn how to use **`ColumnTransformer`** to apply different transformations to different data columns.\n",
        "*   Build **modular and reusable preprocessing pipelines** for combined data.\n",
        "*   Create **clean and efficient code** for feature engineering and model training.\n",
        "*   Experiment with various **transformers and models** within pipelines.\n",
        "\n",
        "Let's dive into how pipelines and feature transformations can streamline our combined data processing!"
      ],
      "metadata": {
        "id": "w721V0AGTwb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAQmRr8QTtao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Pipelines and Feature Transformations? - Structure and Efficiency\n",
        "\n",
        "Why focus on Pipelines and Feature Transformations specifically?  They are not just tools; they represent a **structured and efficient approach** to machine learning workflows, especially when dealing with complex data like combined text and numerical features.\n",
        "\n",
        "**Benefits of Pipelines:**\n",
        "\n",
        "*   **Workflow Organization:** Pipelines encapsulate the entire sequence of preprocessing steps and the model into a single, coherent unit. This makes your code:\n",
        "    *   **More Readable:**  The flow of data from preprocessing to model is clearly defined.\n",
        "    *   **More Maintainable:** Changes to preprocessing or the model are easier to manage within the pipeline structure.\n",
        "    *   **Less Error-Prone:** Reduces the risk of data leakage and inconsistencies between training and testing data.\n",
        "\n",
        "*   **Simplified Training and Prediction:**  Once a pipeline is defined, training and prediction become incredibly simple:\n",
        "    *   **`.fit(X_train, y_train)`**: Train the entire preprocessing and modeling pipeline on the training data.\n",
        "    *   **`.predict(X_test)` or `.transform(X_test)`**: Apply the *entire* preprocessing pipeline to new data (test or new input) *before* feeding it to the model. This ensures consistent preprocessing.\n",
        "\n",
        "*   **Cross-validation and Grid Search Compatibility:** Pipelines work seamlessly with scikit-learn's cross-validation and hyperparameter tuning tools (`GridSearchCV`, `RandomizedSearchCV`). This makes it easy to optimize the entire workflow, including preprocessing steps and model parameters, together.\n",
        "\n",
        "**Benefits of Feature Transformations:**\n",
        "\n",
        "*   **Data Preparation:** Feature transformations are essential for preparing data for machine learning algorithms. They handle:\n",
        "    *   **Scaling Numerical Features:**  `StandardScaler`, `MinMaxScaler`, etc., bring numerical features to a comparable scale, improving algorithm performance.\n",
        "    *   **Vectorizing Text:** `TfidfVectorizer`, `CountVectorizer` convert text into numerical representations that models can understand.\n",
        "    *   **Handling Different Data Types:**  `ColumnTransformer` allows you to apply *different* transformations to *different* columns based on their data type (numerical, text, categorical, etc.).\n",
        "\n",
        "*   **Improved Model Performance:**  Appropriate feature transformations can significantly boost model accuracy and generalization by:\n",
        "    *   Making features more suitable for the chosen model.\n",
        "    *   Reducing the impact of irrelevant or noisy features.\n",
        "    *   Highlighting important patterns in the data.\n",
        "\n",
        "In this lesson, we'll build upon the synthetic dataset from the previous lesson, but this time, we'll explicitly emphasize the use of Pipelines and Feature Transformations to create a more robust and streamlined workflow. Let's start by revisiting the dataset and then building a pipeline step-by-step."
      ],
      "metadata": {
        "id": "SMH9cL-6T5Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 1: Revisiting Synthetic Dataset Generation (Same as Before)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 2. Number of samples\n",
        "n_samples = 500\n",
        "\n",
        "# 3. Generate synthetic numerical features (e.g., 'price', 'popularity')\n",
        "price = np.random.uniform(10, 100, n_samples) # Price range from 10 to 100\n",
        "popularity = np.random.randint(0, 1000, n_samples) # Popularity score from 0 to 1000\n",
        "\n",
        "# 4. Generate synthetic text reviews (simplified - categories influence text)\n",
        "categories = ['electronics', 'clothing', 'books', 'home_decor']\n",
        "category_options = np.random.choice(categories, n_samples)\n",
        "\n",
        "def generate_review_text(category):\n",
        "    if category == 'electronics':\n",
        "        keywords = ['device', 'battery', 'screen', 'performance', 'camera', 'sound', 'quality', 'fast', 'recommend', 'great']\n",
        "    elif category == 'clothing':\n",
        "        keywords = ['fabric', 'fit', 'size', 'comfortable', 'style', 'color', 'soft', 'wear', 'love', 'perfect']\n",
        "    elif category == 'books':\n",
        "        keywords = ['story', 'characters', 'plot', 'reading', 'author', 'recommend', 'enjoyed', 'interesting', 'page', 'written']\n",
        "    elif category == 'home_decor':\n",
        "        keywords = ['decor', 'design', 'style', 'room', 'color', 'beautiful', 'quality', 'look', 'home', 'recommend']\n",
        "    else:\n",
        "        keywords = ['product', 'good', 'nice', 'like', 'recommend'] # Default keywords\n",
        "\n",
        "    review_length = np.random.randint(10, 30) # Review length (words)\n",
        "    review_text = ' '.join(np.random.choice(keywords, review_length)) # Create review by randomly picking keywords\n",
        "    return review_text\n",
        "\n",
        "review_text_data = [generate_review_text(cat) for cat in category_options]\n",
        "\n",
        "# 5. Generate synthetic ratings (numerical target - for regression or classification example)\n",
        "ratings = []\n",
        "for cat in category_options:\n",
        "    if cat == 'electronics':\n",
        "        ratings.append(np.random.normal(4.0, 0.8)) # Electronics tend to have slightly higher ratings\n",
        "    elif cat == 'clothing':\n",
        "        ratings.append(np.random.normal(3.5, 1.0))\n",
        "    elif cat == 'books':\n",
        "        ratings.append(np.random.normal(4.2, 0.7)) # Books often get good ratings\n",
        "    elif cat == 'home_decor':\n",
        "        ratings.append(np.random.normal(3.8, 0.9))\n",
        "    else:\n",
        "        ratings.append(np.random.normal(3.7, 1.0))\n",
        "\n",
        "ratings = np.clip(ratings, 1, 5).round(1) # Clip ratings to be between 1 and 5 and round to 1 decimal place\n",
        "\n",
        "# 6. Create Pandas DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'review_text': review_text_data,\n",
        "    'price': price,\n",
        "    'popularity': popularity,\n",
        "    'category': category_options, # Category (optional - for classification example)\n",
        "    'rating': ratings # Numerical target variable (e.g., for regression or classification)\n",
        "})\n",
        "\n",
        "# 7. Display first few rows of the DataFrame\n",
        "print(\"Sample of Synthetic Dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# 8. Display data types and summary statistics\n",
        "print(\"\\nData Types and Summary Statistics:\")\n",
        "print(data.info()) # Data types\n",
        "print(data.describe()) # Summary statistics for numerical columns"
      ],
      "metadata": {
        "id": "N6teWMcbUBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Combined Feature Pipeline - Step-by-Step with `ColumnTransformer`\n",
        "\n",
        "Let's now construct a scikit-learn Pipeline to process our combined text and numerical data. We'll use `ColumnTransformer` as the core preprocessing step within the pipeline.\n",
        "\n",
        "**Pipeline Construction Steps:**\n",
        "\n",
        "1.  **Define Feature Columns:**  First, we clearly identify our numerical and text feature columns:\n",
        "\n",
        "    ```python\n",
        "    numerical_features = ['price', 'popularity']\n",
        "    text_feature = 'review_text'\n",
        "    ```\n",
        "\n",
        "2.  **Create a `ColumnTransformer`**:  This is where we specify the transformations for each feature type:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features), # Numerical features: StandardScaler\n",
        "            ('text', TfidfVectorizer(stop_words='english', max_features=5000), text_feature) # Text feature: TfidfVectorizer\n",
        "        ])\n",
        "    ```\n",
        "\n",
        "    *   **`transformers=[...]`**:  A list of transformation steps.\n",
        "    *   **`('num', StandardScaler(), numerical_features)`**:  Applies `StandardScaler()` to the columns listed in `numerical_features`. The transformer is named 'num'.\n",
        "    *   **`('text', TfidfVectorizer(stop_words='english', max_features=5000), text_feature)`**: Applies `TfidfVectorizer(...)` to the column `text_feature`. The transformer is named 'text'.\n",
        "\n",
        "3.  **Create a `Pipeline`**:  We assemble the `ColumnTransformer` and a model into a pipeline:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.ensemble import RandomForestRegressor # Example model\n",
        "\n",
        "    combined_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor), # Step 1: ColumnTransformer for preprocessing\n",
        "        ('regressor', RandomForestRegressor(random_state=42)) # Step 2: RandomForestRegressor model\n",
        "    ])\n",
        "    ```\n",
        "\n",
        "    *   **`Pipeline([...])`**: Creates a pipeline as a list of steps.\n",
        "    *   **`('preprocessor', preprocessor)`**:  The first step is our `ColumnTransformer` named 'preprocessor'.\n",
        "    *   **`('regressor', RandomForestRegressor(random_state=42))`**: The second step is a `RandomForestRegressor` model (you can replace this with other models). It's named 'regressor'.\n",
        "\n",
        "4.  **Train the Pipeline:**  Training is now done on the *entire pipeline*:\n",
        "\n",
        "    ```python\n",
        "    X = data[[text_feature] + numerical_features] # Feature DataFrame\n",
        "    y = data['rating'] # Target variable\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    combined_pipeline.fit(X_train, y_train) # Train the ENTIRE pipeline\n",
        "    ```\n",
        "\n",
        "    *   We fit the `combined_pipeline` on the training data `(X_train, y_train)`.  The `fit()` method automatically applies the `preprocessor` (feature transformations) and then trains the `regressor` (model) in sequence.\n",
        "\n",
        "5.  **Make Predictions and Evaluate:**  Predictions are also made using the *entire pipeline*:\n",
        "\n",
        "    ```python\n",
        "    y_pred_combined = combined_pipeline.predict(X_test) # Predict using the ENTIRE pipeline\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "    mse = mean_squared_error(y_test, y_pred_combined)\n",
        "    r2 = r2_score(y_test, y_pred_combined)\n",
        "\n",
        "    print(\"Combined Feature Pipeline - Regression Performance:\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"R-squared (R2): {r2:.4f}\")\n",
        "    ```\n",
        "\n",
        "    *   `combined_pipeline.predict(X_test)`:  This automatically applies the *same* preprocessing steps defined in the `ColumnTransformer` to the `X_test` data *before* making predictions with the trained `RandomForestRegressor`. This ensures consistency and avoids data leakage.\n",
        "\n",
        "Run the code below to see the complete pipeline implementation and evaluation.  Notice how clean and concise the code becomes when using pipelines!"
      ],
      "metadata": {
        "id": "SU8oQhU4UPgq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ogUHnuHUYZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Feature Transformations - Customization within the Pipeline\n",
        "\n",
        "The power of pipelines lies in their flexibility. Let's explore how we can easily modify and customize the feature transformations within our `ColumnTransformer` and pipeline.\n",
        "\n",
        "**Experimenting with Text Vectorization:**\n",
        "\n",
        "*   **Change `TfidfVectorizer` parameters:**  Let's say you want to experiment with different `ngram_range` values for the text feature.  You can directly modify the `TfidfVectorizer` within the `ColumnTransformer` definition:\n",
        "\n",
        "    ```python\n",
        "    preprocessor_ngram = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('text_ngram', TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2)), text_feature) # Experiment with ngrams (unigrams and bigrams)\n",
        "        ])\n",
        "\n",
        "    pipeline_ngram = Pipeline([\n",
        "        ('preprocessor', preprocessor_ngram),\n",
        "        ('regressor', RandomForestRegressor(random_state=42))\n",
        "    ])\n",
        "\n",
        "    pipeline_ngram.fit(X_train, y_train) # Train the new pipeline\n",
        "    y_pred_ngram = pipeline_ngram.predict(X_test) # Predict\n",
        "    # ... evaluate pipeline_ngram ...\n",
        "    ```\n",
        "\n",
        "    Here, we created a *new* `ColumnTransformer` called `preprocessor_ngram` where we changed `TfidfVectorizer` to use `ngram_range=(1, 2)` (unigrams and bigrams). We then created a new pipeline `pipeline_ngram` using this modified preprocessor.  This demonstrates how easy it is to experiment with different vectorizer settings.\n",
        "\n",
        "*   **Try `CountVectorizer`:**  To compare `TfidfVectorizer` with `CountVectorizer`, simply replace `TfidfVectorizer` with `CountVectorizer` in the `ColumnTransformer`:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    preprocessor_bow = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('text_bow', CountVectorizer(stop_words='english', max_features=5000), text_feature) # Use CountVectorizer instead of TfidfVectorizer\n",
        "        ])\n",
        "\n",
        "    pipeline_bow = Pipeline([\n",
        "        ('preprocessor', preprocessor_bow),\n",
        "        ('regressor', RandomForestRegressor(random_state=42))\n",
        "    ])\n",
        "\n",
        "    pipeline_bow.fit(X_train, y_train) # Train pipeline_bow\n",
        "    y_pred_bow = pipeline_bow.predict(X_test) # Predict\n",
        "    # ... evaluate pipeline_bow ...\n",
        "    ```\n",
        "\n",
        "**Experimenting with Numerical Scaling:**\n",
        "\n",
        "*   **Try `MinMaxScaler`:**  To use `MinMaxScaler` instead of `StandardScaler` for numerical features, just change the transformer in the `ColumnTransformer`:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    preprocessor_minmax = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num_minmax', MinMaxScaler(), numerical_features), # Use MinMaxScaler\n",
        "            ('text', TfidfVectorizer(stop_words='english', max_features=5000), text_feature)\n",
        "        ])\n",
        "\n",
        "    pipeline_minmax = Pipeline([\n",
        "        ('preprocessor', preprocessor_minmax),\n",
        "        ('regressor', RandomForestRegressor(random_state=42))\n",
        "    ])\n",
        "\n",
        "    pipeline_minmax.fit(X_train, y_train) # Train pipeline_minmax\n",
        "    y_pred_minmax = pipeline_minmax.predict(X_test) # Predict\n",
        "    # ... evaluate pipeline_minmax ...\n",
        "    ```\n",
        "\n",
        "**Key Idea:** Pipelines and `ColumnTransformer` make it very easy to swap out and experiment with different preprocessing steps and models. You can create different pipelines with variations in feature transformations and models and then compare their performance systematically.\n",
        "\n",
        "Run the code cell below to see examples of creating pipelines with different vectorizers and scalers and evaluating their performance."
      ],
      "metadata": {
        "id": "_xiL1YROUfqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: Pipelines with Different Feature Transformations (Vectorizers, Scalers)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler # Import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 2. Define features and target, and split data (same as before)\n",
        "numerical_features = ['price', 'popularity']\n",
        "text_feature = 'review_text'\n",
        "X = data[[text_feature] + numerical_features]\n",
        "y = data['rating']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Define different preprocessing pipelines using ColumnTransformer\n",
        "\n",
        "# Pipeline 1: StandardScaler + TfidfVectorizer (Baseline - same as before)\n",
        "preprocessor_tfidf_scaler = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('text', TfidfVectorizer(stop_words='english', max_features=5000), text_feature)\n",
        "    ])\n",
        "pipeline_tfidf_scaler = Pipeline([\n",
        "    ('preprocessor', preprocessor_tfidf_scaler),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Pipeline 2: MinMaxScaler + TfidfVectorizer\n",
        "preprocessor_tfidf_minmax = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numerical_features), # Use MinMaxScaler instead of StandardScaler\n",
        "        ('text', TfidfVectorizer(stop_words='english', max_features=5000), text_feature)\n",
        "    ])\n",
        "pipeline_tfidf_minmax = Pipeline([\n",
        "    ('preprocessor', preprocessor_tfidf_minmax),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Pipeline 3: StandardScaler + CountVectorizer\n",
        "preprocessor_bow_scaler = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('text', CountVectorizer(stop_words='english', max_features=5000), text_feature) # Use CountVectorizer instead of TfidfVectorizer\n",
        "    ])\n",
        "pipeline_bow_scaler = Pipeline([\n",
        "    ('preprocessor', preprocessor_bow_scaler),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "# 4. Train and evaluate each pipeline\n",
        "pipelines = { # Store pipelines and their names for easy iteration\n",
        "    'Pipeline_TFIDF_Scaler': pipeline_tfidf_scaler,\n",
        "    'Pipeline_TFIDF_MinMaxScaler': pipeline_tfidf_minmax,\n",
        "    'Pipeline_BOW_Scaler': pipeline_bow_scaler\n",
        "}\n",
        "performance = {} # Store performance metrics\n",
        "\n",
        "for name, pipeline in pipelines.items():\n",
        "    pipeline.fit(X_train, y_train) # Train pipeline\n",
        "    y_pred = pipeline.predict(X_test) # Predict\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    performance[name] = {'MSE': mse, 'R2': r2} # Store metrics\n",
        "\n",
        "# 5. Print performance comparison\n",
        "print(\"Performance Comparison of Different Pipelines:\")\n",
        "for name, metrics in performance.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  MSE: {metrics['MSE']:.4f}\")\n",
        "    print(f\"  R-squared (R2): {metrics['R2']:.4f}\")"
      ],
      "metadata": {
        "id": "cuzPVKmvUj47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning for Pipelines - Optimizing the Entire Workflow\n",
        "\n",
        "Pipelines become even more powerful when combined with hyperparameter tuning. We can use `GridSearchCV` (or `RandomizedSearchCV`) to automatically search for the best combination of hyperparameters for both the preprocessing steps and the model *within* the pipeline.\n",
        "\n",
        "**Tuning Pipeline Hyperparameters with `GridSearchCV`:**\n",
        "\n",
        "1.  **Define Parameter Grid:** Create a `param_grid` dictionary where keys are *pipeline step names* followed by `__` and then the hyperparameter name. Values are lists of hyperparameter values to try.\n",
        "\n",
        "    ```python\n",
        "    param_grid = {\n",
        "        'preprocessor__text__max_features': [1000, 5000, 10000], # Tune max_features in TfidfVectorizer (text preprocessing step)\n",
        "        'preprocessor__text__ngram_range': [(1, 1), (1, 2)], # Tune ngram_range in TfidfVectorizer\n",
        "        'regressor__n_estimators': [100, 200, 500], # Tune n_estimators in RandomForestRegressor (model step)\n",
        "        'regressor__max_depth': [None, 10, 20] # Tune max_depth in RandomForestRegressor\n",
        "    }\n",
        "    ```\n",
        "\n",
        "    *   **`'preprocessor__text__max_features'`**:  Targets the `max_features` parameter of the `TfidfVectorizer` which is part of the 'text' transformer within the 'preprocessor' step of the pipeline.  The `__` (double underscore) notation is used to access nested parameters within pipeline steps.\n",
        "    *   Similarly, `'regressor__n_estimators'` and `'regressor__max_depth'` target hyperparameters of the `RandomForestRegressor` model step.\n",
        "\n",
        "2.  **Initialize `GridSearchCV` with the Pipeline and Parameter Grid:**\n",
        "\n",
        "    ```python\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline_tfidf_scaler, # Pipeline to tune (e.g., pipeline_tfidf_scaler from previous example)\n",
        "                               param_grid, # Parameter grid\n",
        "                               cv=3, # Cross-validation folds (e.g., 3-fold CV)\n",
        "                               scoring='neg_mean_squared_error', # Scoring metric for regression (negative MSE for GridSearchCV to maximize)\n",
        "                               n_jobs=-1) # Use all available CPU cores for parallel processing (optional)\n",
        "    ```\n",
        "\n",
        "    *   We pass the pipeline (`pipeline_tfidf_scaler` in this example, you can use any pipeline you defined) and the `param_grid` to `GridSearchCV`.\n",
        "    *   `cv=3`:  Specifies 3-fold cross-validation.\n",
        "    *   `scoring='neg_mean_squared_error'`:  We use negative mean squared error as the scoring metric for regression. `GridSearchCV` maximizes the score, so we use *negative* MSE because we want to *minimize* MSE.\n",
        "    *   `n_jobs=-1`:  Optional, uses all CPU cores for faster grid search.\n",
        "\n",
        "3.  **Fit `GridSearchCV`:**\n",
        "\n",
        "    ```python\n",
        "    grid_search.fit(X_train, y_train) # Fit GridSearchCV on training data\n",
        "    ```\n",
        "\n",
        "    *   `grid_search.fit()` performs cross-validation for all combinations of hyperparameters in `param_grid` and finds the best combination based on the scoring metric.\n",
        "\n",
        "4.  **Get Best Pipeline and Results:**\n",
        "\n",
        "    ```python\n",
        "    best_pipeline = grid_search.best_estimator_ # Get the best pipeline from GridSearchCV\n",
        "    best_params = grid_search.best_params_ # Get the best hyperparameters\n",
        "    best_score = grid_search.best_score_ # Get the best cross-validation score (negative MSE)\n",
        "\n",
        "    print(\"Best Pipeline from GridSearchCV:\")\n",
        "    print(best_pipeline) # Print the best pipeline (with best hyperparameters set)\n",
        "    print(\"\\nBest Hyperparameters:\", best_params) # Print best hyperparameters found\n",
        "    print(f\"\\nBest Cross-Validation Score (Negative MSE): {best_score:.4f}\") # Print best CV score\n",
        "\n",
        "    y_pred_best = best_pipeline.predict(X_test) # Make predictions with the best pipeline on test data\n",
        "    mse_best = mean_squared_error(y_test, y_pred_best) # Evaluate on test data\n",
        "    r2_best = r2_score(y_test, y_pred_best)\n",
        "    print(f\"\\nPerformance on Test Data (Best Pipeline):\")\n",
        "    print(f\"MSE: {mse_best:.4f}\")\n",
        "    print(f\"R-squared (R2): {r2_best:.4f}\")\n",
        "    ```\n",
        "\n",
        "    *   `grid_search.best_estimator_`:  Retrieves the trained pipeline with the best hyperparameter combination found by `GridSearchCV`.\n",
        "    *   `grid_search.best_params_`:  Gets the dictionary of best hyperparameters.\n",
        "    *   `grid_search.best_score_`:  Gets the best cross-validation score achieved (negative MSE in this case).\n",
        "    *   We then evaluate the `best_pipeline` on the test data to get an estimate of its generalization performance.\n",
        "\n",
        "Run the code cell below to see an example of using `GridSearchCV` to tune a combined feature pipeline.  Hyperparameter tuning can often lead to significant improvements in model performance!"
      ],
      "metadata": {
        "id": "BkyIjworUx_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 4: Pipeline Hyperparameter Tuning with GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV # Import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 2. Define features and target, and split data (same as before)\n",
        "numerical_features = ['price', 'popularity']\n",
        "text_feature = 'review_text'\n",
        "X = data[[text_feature] + numerical_features]\n",
        "y = data['rating']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Define the pipeline (using StandardScaler and TfidfVectorizer as baseline)\n",
        "preprocessor_tune = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('text', TfidfVectorizer(stop_words='english'), text_feature) # Keep vectorizer params simple for tuning example\n",
        "    ])\n",
        "pipeline_tune = Pipeline([\n",
        "    ('preprocessor', preprocessor_tune),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# 4. Define the parameter grid to tune\n",
        "param_grid = {\n",
        "    'preprocessor__text__max_features': [1000, 5000, 10000], # Tune max_features in TfidfVectorizer\n",
        "    'preprocessor__text__ngram_range': [(1, 1), (1, 2)], # Tune ngram_range in TfidfVectorizer\n",
        "    'regressor__n_estimators': [100, 200, 500], # Tune n_estimators in RandomForestRegressor\n",
        "    'regressor__max_depth': [None, 10, 20] # Tune max_depth in RandomForestRegressor\n",
        "}\n",
        "\n",
        "# 5. Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline_tune,\n",
        "                           param_grid,\n",
        "                           cv=3,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           n_jobs=-1)\n",
        "\n",
        "# 6. Fit GridSearchCV (this will take some time)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Get best pipeline, parameters, and score\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Pipeline from GridSearchCV:\")\n",
        "print(best_pipeline)\n",
        "print(\"\\nBest Hyperparameters:\", best_params)\n",
        "print(f\"\\nBest Cross-Validation Score (Negative MSE): {best_score:.4f}\")\n",
        "\n",
        "# 8. Evaluate best pipeline on test data\n",
        "y_pred_best = best_pipeline.predict(X_test)\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "print(f\"\\nPerformance on Test Data (Best Pipeline):\")\n",
        "print(f\"MSE: {mse_best:.4f}\")\n",
        "print(f\"R-squared (R2): {r2_best:.4f}\")"
      ],
      "metadata": {
        "id": "m5pYza1bUysU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimentation Prompts - Pipelines and Feature Transformation\n",
        "\n",
        "Now it's your turn to experiment and solidify your understanding of pipelines and feature transformations! Try these:\n",
        "\n",
        "1.  **Explore Different Models in Pipelines:**\n",
        "    *   Replace `RandomForestRegressor` in your pipelines with other regression models: `LinearRegression`, `Ridge`, `Lasso`, `GradientBoostingRegressor`, `SVR`.\n",
        "    *   For each model, compare the performance with and without hyperparameter tuning using `GridSearchCV`.\n",
        "    *   Which models work best with combined features within a pipeline framework?\n",
        "\n",
        "2.  **Customize Numerical Transformations:**\n",
        "    *   Experiment with different numerical scalers in your `ColumnTransformer`: `MinMaxScaler`, `RobustScaler`, `PowerTransformer`, `QuantileTransformer`.\n",
        "    *   For each scaler, evaluate the pipeline performance (with and without tuning).\n",
        "    *   Does the choice of numerical scaler significantly impact the results for different models?\n",
        "\n",
        "3.  **Advanced Text Preprocessing in Pipelines:**\n",
        "    *   Integrate more advanced text preprocessing steps into your `TfidfVectorizer` within the pipeline:\n",
        "        *   **Stemming or Lemmatization:** Create a custom transformer or use libraries like `nltk` or `spaCy` to add stemming/lemmatization within the text preprocessing step of your pipeline.\n",
        "        *   **Custom Stop Word Lists:** Experiment with different stop word lists or create your own custom stop word list relevant to your dataset.\n",
        "        *   **Character n-grams:** Try `analyzer='char_wb'` or `analyzer='char'` in `TfidfVectorizer` and explore character n-grams instead of word n-grams.\n",
        "    *   How do these advanced text preprocessing techniques affect pipeline performance?\n",
        "\n",
        "4.  **Pipeline for Classification (Optional):**\n",
        "    *   Adapt the synthetic dataset or use a new dataset for a *classification* task (e.g., sentiment classification, document categorization). You can use the `category` column in our synthetic data as a classification target.\n",
        "    *   Modify your pipeline to use classification models (e.g., `LogisticRegression`, `RandomForestClassifier`, `SVC`).\n",
        "    *   Evaluate classification performance using appropriate metrics (accuracy, classification report, confusion matrix).\n",
        "\n",
        "5.  **Real-World Dataset Pipeline (Challenge):**\n",
        "    *   Choose a real-world dataset with combined text and numerical features.\n",
        "    *   Build a complete pipeline using `ColumnTransformer`, feature transformations, and a model to solve a relevant prediction task on this dataset.\n",
        "    *   Perform hyperparameter tuning using `GridSearchCV` to optimize your pipeline.\n",
        "    *   Document your pipeline, feature engineering choices, and results.\n",
        "\n",
        "Think about these questions during your experiments:\n",
        "\n",
        "*   How do pipelines simplify the process of trying different preprocessing and modeling approaches?\n",
        "*   What are the most important hyperparameters to tune in your combined feature pipelines?\n",
        "*   Do pipelines make your machine learning workflows more robust and reproducible?\n",
        "*   What are the benefits and challenges of using pipelines in real-world projects?\n",
        "\n",
        "After your experiments, review the summary and key takeaways for this lesson."
      ],
      "metadata": {
        "id": "_CfeVd4rVAdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deeper Dive - Pipeline Flexibility and Custom Transformers\n",
        "\n",
        "Pipelines are designed to be highly flexible and extensible. Let's explore some advanced aspects:\n",
        "\n",
        "*   **Custom Transformers:**  You are not limited to using only scikit-learn's built-in transformers within pipelines. You can create your own **custom transformers** to perform specific preprocessing steps that are not directly available in scikit-learn.\n",
        "\n",
        "    *   **Creating Custom Transformers:**  To create a custom transformer, you need to define a Python class that inherits from `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin` and implements `fit()` and `transform()` methods.\n",
        "\n",
        "    *   **Example - Simple Custom Transformer (for demonstration):**\n",
        "\n",
        "        ```python\n",
        "        from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "        class TextLengthExtractor(BaseEstimator, TransformerMixin): # Inherit from BaseEstimator and TransformerMixin\n",
        "            def __init__(self):\n",
        "                pass # No parameters to initialize in this simple example\n",
        "\n",
        "            def fit(self, X, y=None):\n",
        "                return self # `fit` method usually does nothing in simple transformers\n",
        "\n",
        "            def transform(self, X):\n",
        "                return [[len(text)] for text in X] # Transform: return text length as a 2D array\n",
        "        ```\n",
        "\n",
        "        This simple custom transformer `TextLengthExtractor` calculates the length of text documents. You could create more complex custom transformers for tasks like:\n",
        "            *   Applying specific text cleaning steps (e.g., custom regex-based cleaning).\n",
        "            *   Feature engineering steps that are specific to your domain.\n",
        "            *   Integrating external libraries or tools into your preprocessing workflow.\n",
        "\n",
        "    *   **Using Custom Transformers in Pipelines:**  You can use your custom transformers just like any other scikit-learn transformer within a `ColumnTransformer` and a pipeline:\n",
        "\n",
        "        ```python\n",
        "        preprocessor_custom = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', StandardScaler(), numerical_features),\n",
        "                ('text_length', TextLengthExtractor(), text_feature), # Use custom TextLengthExtractor\n",
        "                ('text_tfidf', TfidfVectorizer(stop_words='english', max_features=5000), text_feature) # Still use TF-IDF as well\n",
        "            ])\n",
        "\n",
        "        pipeline_custom = Pipeline([\n",
        "            ('preprocessor', preprocessor_custom),\n",
        "            ('regressor', RandomForestRegressor(random_state=42))\n",
        "        ])\n",
        "        ```\n",
        "\n",
        "*   **Feature Union (Optional - More Advanced):** For even more complex feature engineering scenarios, you can use `FeatureUnion` (from `sklearn.pipeline`) to combine the outputs of *multiple* transformers applied to the *same* set of columns.  This is useful when you want to generate multiple feature representations from the same input data and combine them.\n",
        "\n",
        "Pipelines and custom transformers provide a powerful and flexible framework for building sophisticated machine learning workflows, allowing you to tailor your preprocessing and modeling steps precisely to the needs of your data and problem."
      ],
      "metadata": {
        "id": "S27OLV2iVJnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary and Next Steps - Pipeline Power and Transformation Mastery\n",
        "\n",
        "Fantastic work on mastering pipelines and feature transformations for combined data! In this lesson, you've:\n",
        "\n",
        "*   Reinforced your understanding of **scikit-learn Pipelines** for organized workflows.\n",
        "*   Focused on **Feature Transformations** as key preprocessing steps within pipelines.\n",
        "*   Used **`ColumnTransformer`** to apply different transformations to different columns effectively.\n",
        "*   Built **modular and reusable preprocessing pipelines**.\n",
        "*   Streamlined your code and workflow for combined feature learning.\n",
        "*   Experimented with various **transformers, models, and hyperparameter tuning** within pipelines.\n",
        "\n",
        "**Key Takeaways for Pipelines and Feature Transformations:**\n",
        "\n",
        "*   **Pipelines are essential for building robust, organized, and reproducible machine learning workflows.**\n",
        "*   **`ColumnTransformer` is the key to handling datasets with mixed feature types within pipelines.**\n",
        "*   **Feature transformations (scaling, vectorization) are crucial preprocessing steps** for preparing data for effective modeling.\n",
        "*   **Pipelines simplify training, prediction, and hyperparameter tuning** for complex workflows.\n",
        "*   **Custom transformers extend the flexibility of pipelines** to handle specialized preprocessing tasks.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "In our upcoming lessons, we'll continue to leverage pipelines and feature transformations as we explore even more advanced topics:\n",
        "\n",
        "*   **Word Embeddings and Pre-trained Language Models:** Integrating more sophisticated text representations into pipelines.\n",
        "*   **End-to-End Machine Learning Projects:**  Applying pipelines and feature engineering techniques to solve real-world problems from data loading to model deployment.\n",
        "*   **Advanced Model Architectures:**  Building more complex models, potentially including neural networks, within pipeline workflows.\n",
        "\n",
        "You are now equipped with a powerful and structured approach to machine learning with combined data!  Continue practicing and building more complex pipelines to tackle increasingly challenging problems!"
      ],
      "metadata": {
        "id": "lCNHnHi3VO19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways for Lesson #3d (Pipelines & Transformations Focus):\n",
        "\n",
        "*   **Pipelines organize ML workflows, improve readability and prevent data leakage.**\n",
        "*   **`ColumnTransformer` is central to pipelines for mixed data types.**\n",
        "*   **Feature transformations (scalers, vectorizers) are crucial preprocessing steps.**\n",
        "*   **Pipelines streamline training, prediction, and hyperparameter tuning.**\n",
        "*   **Custom transformers extend pipeline flexibility.**\n",
        "*   Master pipelines for robust and efficient machine learning workflows."
      ],
      "metadata": {
        "id": "jKQlg48aVSkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resources for Lesson #3d (Pipelines & Transformations Focus):\n",
        "\n",
        "*   **Scikit-learn documentation on Pipelines:** [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
        "*   **Scikit-learn documentation on `ColumnTransformer`:** [https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
        "*   **Scikit-learn documentation on Feature Scaling:** [https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
        "*   **Scikit-learn documentation on Text Feature Extraction:** [https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
        "*   **Scikit-learn documentation on Custom Transformers:** [https://scikit-learn.org/stable/developers/develop.html#transformers](https://scikit-learn.org/stable/developers/develop.html#transformers) (Developer guide section on Transformers)"
      ],
      "metadata": {
        "id": "s3LQOQliVYjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Notes - Pipelines and Workflow Best Practices:\n",
        "\n",
        "*   **Pipeline Step Naming:** Use descriptive names for pipeline steps (e.g., 'preprocessor', 'regressor', 'tfidf_vectorizer', 'numerical_scaler'). This makes your pipeline definition more readable and helps with hyperparameter tuning (referencing steps by name in `param_grid`).\n",
        "\n",
        "*   **Immutability:** Pipelines are designed to be immutable after creation. If you need to change a pipeline, create a *new* pipeline with the modifications rather than trying to alter an existing one in place. This promotes clarity and avoids unexpected side effects.\n",
        "\n",
        "*   **Data Exploration Before Pipeline:** While pipelines handle preprocessing and modeling, it's still essential to perform initial data exploration *before* building your pipeline. Understand your data types, distributions, potential issues (missing values, outliers), and inform your feature engineering and pipeline design based on this exploration.\n",
        "\n",
        "*   **Modular Pipeline Design:** Break down complex preprocessing workflows into smaller, modular transformers and pipeline steps. This makes your pipelines easier to understand, test, and reuse in different projects.\n",
        "\n",
        "*   **Testing and Validation:**  Thoroughly test your pipelines, especially custom transformers, to ensure they are working as expected. Use unit tests or simple validation datasets to check the output of each pipeline step.\n",
        "\n",
        "*   **Version Control:** Use version control (like Git) to track changes to your pipelines and feature engineering code. This is crucial for reproducibility and collaboration in machine learning projects.\n",
        "\n",
        "*   **Documentation:** Document your pipelines clearly, explaining the purpose of each step, the transformations applied, and any important design decisions. Good documentation is essential for maintainability and for others (or your future self) to understand and reuse your workflows."
      ],
      "metadata": {
        "id": "UMQrEg9kVgUj"
      }
    }
  ]
}
